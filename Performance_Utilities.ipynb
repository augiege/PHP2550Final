{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix,roc_curve, auc, roc_auc_score, accuracy_score, classification_report,\\\n",
    "cohen_kappa_score, f1_score\n",
    "from sklearn.base import clone\n",
    "\n",
    "from scipy.stats.mstats import gmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _risk_score(likelihoods, y):\n",
    "    '''\n",
    "    This function takes the probability predictions from the model and transforms them into incidence-rate weighted\n",
    "    risk scores. We bucket patients based on the percentile value predicted by the model and then calculate the rate of \n",
    "    complication for patients in that bucket. We also calculate the standard dev and 95% CI on the distribution of \n",
    "    probability values within each bucket.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    likelihoods - numpy array of probability percentages\n",
    "    y - pandas dataframe, Outcome from data\n",
    "    \n",
    "    Output\n",
    "    ----------\n",
    "    risk_values - array of weighted risk values\n",
    "    '''\n",
    "    bucket_bounds_by_tens = list(range(0,101,10))\n",
    "    percentiles = np.percentile(likelihoods, bucket_bounds_by_tens)\n",
    "    risk_values = np.copy(likelihoods)\n",
    "    \n",
    "    for i in range(10):\n",
    "        patients = np.where((likelihoods>=percentiles[i]) & (likelihoods<=percentiles[i+1]))\n",
    "        rate = y[patients[0]].mean()\n",
    "        risk_values[patients[0]] = rate\n",
    "    \n",
    "    return risk_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_optimal_threshold(X, y, classifier, title):\n",
    "    \n",
    "    '''\n",
    "    This function creates ROC plot on validation set for each fold in 5-fold cross-validation using training dataset \n",
    "    to visualize variation among different folds. ROC curve of the mean of 5 folds is shown in blue \n",
    "    while the chance (0.5) is shown in the diagonal dash line. Also, the optimal threshold is found \n",
    "    by taking average of the optimal threshold identified in each fold in cross-validation and returned.\n",
    "    \n",
    "    Reference: https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc_crossval.html#sphx-glr-auto-examples-model-selection-plot-roc-crossval-py\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: numpy array, features from training data\n",
    "    y: numpy array, Outcome from training data\n",
    "    classifier: Trained model, i.e. model.best_estimates_\n",
    "    title: String, Target for model\n",
    "    '''\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "    optimal_thresholds = []\n",
    "    classifier_temp = clone(classifier)\n",
    "\n",
    "    for train, test in cv.split(X, y):\n",
    "        probas_ = classifier_temp.fit(X.iloc[train,:], y.iloc[train]).predict_proba(X.iloc[test,:])\n",
    "\n",
    "        # Compute ROC curve and area under the curve\n",
    "        fpr, tpr, thresholds = roc_curve(y.iloc[test], probas_[:, 1])\n",
    "        \n",
    "        #Geometric mean\n",
    "        optimal_idx = np.argmax(np.sqrt(tpr*(1-fpr)))\n",
    "        optimal_threshold = thresholds[optimal_idx]\n",
    "        optimal_thresholds.append(optimal_threshold)\n",
    "        \n",
    "\n",
    "\n",
    "    return optimal_thresholds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(X_test,y_test,classifier,outcome,file_name=None,file_suffix=None,matrix_color='Greens'):\n",
    "    \n",
    "    '''\n",
    "    This function prints model performance metrics based on the test set using the calculated optimal threshold.\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_test: numpy array, features from test data\n",
    "    y_test: numpy array, Outcome from test data\n",
    "    classifer: The resulting classifer.best_estimator_ that is produced when using sklearn.model_selection.GridSearchCV\n",
    "    optimal_thresholds: Numpy array, Calculated optimal thresholds\n",
    "    outcome: String, Target for model\n",
    "    file_name: string containing relative file path and the name of input file\n",
    "    file_suffix: string containing abbreviations denoting whether the model is continuous or categorical, and peri or no peri\n",
    "    matrix_color: String, Color of the confusion matrix\n",
    "    \n",
    "    Output\n",
    "    ----------\n",
    "    String: Target Name, Accuracy, AUC Score (Test), Sensitivity/True positive rate: \n",
    "    String: Specificity /True negative rate:, False positive rate: , False negative rate: \n",
    "    String: Precision: ,Geometric Mean: ,Cohen's Kappa:, F Score:\n",
    "    Figure: Confusion Matrix \n",
    "    '''\n",
    "    \n",
    "    y_test_prob = classifier.predict_proba(X_test)[:,1]\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    risk_values = _risk_score(y_test_prob, y_test)\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    \n",
    "    Accuracy = round(accuracy_score(y_test, y_pred),3)\n",
    "    AUC =  round(roc_auc_score(y_test, y_test_prob),3)\n",
    "    \n",
    "\n",
    "    TPR = round(tp/(tp+fn),3)\n",
    "    TNR = round(tn/(tn+fp),3)\n",
    "    FPR = round(fp/(tn+fp),3)\n",
    "    FNR = round(fn/(tp+fn),3)\n",
    "    Precision = round(tp/(tp+fp),3)\n",
    "    \n",
    "    geo_mean = round(gmean([TPR,TNR]),3)    \n",
    "    \n",
    "    kappa = round(cohen_kappa_score(y_test, y_pred, weights=None),3)\n",
    "    \n",
    "    f_score = round(f1_score(y_test, y_pred),3)\n",
    "\n",
    "    metrics = [[outcome,Accuracy,TPR,TNR,FPR,FNR,TPR,TNR,AUC,f_score,geo_mean,kappa, sklearn.__version__]]\n",
    "    col_names = ['Model','Accuracy','True Positive Rate','True Negative Rate','False Positive Rate','False Negative Rate',\n",
    "    'Sensitivity','Specificity','AUC','F1-Score','Geometric Mean','Kappa-Statistics', 'Sklearn Version']\n",
    "    model_metrics = pd.DataFrame(metrics,columns=col_names)\n",
    "    \n",
    "    \n",
    "    df_cm = pd.DataFrame(cm, columns=[0,1], index = [0,1])\n",
    "    df_cm.index.name = 'Actual'\n",
    "    df_cm.columns.name = 'Predicted'\n",
    "    plt.figure(figsize = (10,7))\n",
    "    sns.set(font_scale=1.4)#for label size\n",
    "    _ = sns.heatmap(df_cm, cmap=matrix_color, annot=True,fmt='g',annot_kws={\"size\": 16})\n",
    "    plt.title(outcome, fontsize=12)\n",
    "    \n",
    "    if file_name:\n",
    "        plt.savefig(file_name+'_confusion_matrix'+file_suffix)\n",
    "        plt.close()\n",
    "        \n",
    "    return risk_values, model_metrics, AUC, f_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
